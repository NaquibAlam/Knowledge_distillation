{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport os\nimport re\nimport csv\nfrom collections import OrderedDict\n\nimport numpy as np\nimport torch\nfrom torchtext import data\nfrom torchtext.vocab import pretrained_aliases, Vocab\n\nimport spacy\nfrom spacy.symbols import ORTH\nimport argparse\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.nn.utils.rnn import pad_packed_sequence, pack_padded_sequence\nfrom transformers import (BertConfig, BertForSequenceClassification, BertTokenizer)\nfrom tqdm.autonotebook import tqdm, trange\nfrom torch.utils.data import Dataset, DataLoader, RandomSampler, SequentialSampler\nfrom tensorboardX import SummaryWriter","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"spacy_en = spacy.load(\"en\")\nspacy_en.tokenizer.add_special_case(\"<mask>\", [{ORTH: \"<mask>\"}])\nmask_token = \"<mask>\"","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"def set_seed(seed):\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n\n#Interface of this class is similar to torchtext.data.Vocab so that BertVocab can be directly assigned to torchtext.data.Vocab for compatibility.\nclass BertVocab:\n    UNK = '<unk>'\n    def __init__(self, stoi):\n        self.stoi = OrderedDict()\n        #any token inside [] should be replaced by token inside <> for torchtext. \n        #Bert vocab has some special tokens inside [] but torchtext has inside <>.\n        pattern = re.compile(r\"\\[(.*)\\]\") \n        for s, idx in stoi.items():\n            s = s.lower()\n            m = pattern.match(s)\n            if m:\n                content = m.group(1)\n                s = \"<%s>\" % content\n            self.stoi[s] = idx\n        self.unk_index = self.stoi[BertVocab.UNK]\n        self.itos = [(s, idx) for s, idx in self.stoi.items()] \n        self.itos.sort(key=lambda x: x[1])\n        self.itos = [s for (s, idx) in self.itos]\n    def _default_unk_index(self):\n        return self.unk_index\n    def __getitem__(self, token):\n        return self.stoi.get(token, self.stoi.get(BertVocab.UNK))\n    def __len__(self):\n        return len(self.itos)\n\ndef spacy_tokenizer(text):\n    return [tok.text for tok in spacy_en.tokenizer(text)]\n\ndef load_tsv(path, skip_header=True):\n    with open(path) as f:\n        reader = csv.reader(f, delimiter='\\t')\n        if skip_header:\n            next(reader)\n        data = [row for row in reader]\n    return data\n\ndef load_data(data_dir, tokenizer, vocab=None, batch_first=False, augmented=False, use_teacher=False):\n    text_field = data.Field(sequential=True, tokenize=tokenizer, lower=True, include_lengths=True, batch_first=batch_first)\n    label_field_class = data.Field(sequential=False, use_vocab=False, dtype=torch.long)\n    if augmented or use_teacher:\n        # Augmented dataset uses class scores as labels\n        label_field_scores = data.Field(sequential=False, batch_first=True, use_vocab=False,\n            preprocessing=lambda x: [float(n) for n in x.split(\" \")], dtype=torch.float32)\n        fields_train = [(\"text\", text_field), (\"label\", label_field_scores)]\n    else:\n        # Original training set uses the class id\n        fields_train = [(\"text\", text_field), (\"label\", label_field_class)]\n\n    if augmented:\n        train_file = \"augmented.tsv\"\n    elif use_teacher:\n        train_file = \"noaugmented.tsv\"\n    else:\n        train_file = \"train.tsv\"\n    train_dataset = data.TabularDataset(\n        path=os.path.join(data_dir, train_file),\n        format=\"tsv\",  skip_header=True,\n        fields=fields_train\n    )\n\n    fields_valid = [(\"text\", text_field), (\"label\", label_field_class)]\n    valid_dataset = data.TabularDataset(\n        path=os.path.join(data_dir, \"dev.tsv\"),\n        format=\"tsv\", skip_header=True,\n        fields=fields_valid\n    )\n\n    # Initialize field's vocabulary\n    if vocab is None:\n#         vectors = pretrained_aliases[\"fasttext.en.300d\"]()\n        vectors = pretrained_aliases[\"glove.6B.300d\"]() #vectors here basically work as a dict with keys as words and values as 300d embedding vectors.\n        text_field.build_vocab(train_dataset, vectors=vectors) #this will take care of creating word_to_idx dict and embedding matrix (used for initializing weight of nn.Embedding) \n        del vectors\n    else:\n        # Use bert tokenizer's vocab if supplied\n        text_field.vocab = vocab\n    return train_dataset, valid_dataset, text_field\n\ndef get_model_wrapper(model_weights, text_field, device=None):\n    if isinstance(text_field, str):\n        text_field = torch.load(text_field)\n    if isinstance(model_weights, str):\n        model_weights = torch.load(model_weights)\n    if device is None:\n        device = torch.device(\"cpu\")\n\n    vocab = text_field.vocab\n    model = BiLSTMClassifier(2, len(vocab.itos), vocab.vectors.shape[-1],\n        lstm_hidden_size=300, classif_hidden_size=400, dropout_rate=0.15).to(device)\n    model.load_state_dict(model_weights)\n    trainer = LSTMTrainer(model, device)\n    \n    def model_wrapper(text):\n        outputs = trainer.infer_one(text, text_field, softmax=True)\n        return {\n            \"Negative\": outputs[0],\n            \"Positive\": outputs[1]\n        }\n    return model_wrapper\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def build_pos_dict(sentences):\n    \"\"\"\n    creates POS dict which will be used for augmentation as described here https://blog.floydhub.com/knowledge-distillation/.\n    \"\"\"\n    pos_dict = {}\n    for sentence in sentences:\n        for word in sentence:   \n            pos_tag = word.pos_\n            if pos_tag not in pos_dict:\n                pos_dict[pos_tag] = []\n            if word.text.lower() not in pos_dict[pos_tag]:\n                pos_dict[pos_tag].append(word.text.lower())\n    return pos_dict\n\ndef make_sample(input_sentence, pos_dict, p_mask=0.1, p_pos=0.1, p_ng=0.25, max_ng=5):\n    \"\"\"\n    generates augmenetd samples for a given input sentence based on three techniques described here https://blog.floydhub.com/knowledge-distillation/.\n    \"\"\"\n    sentence = []\n    for word in input_sentence:\n        # Apply single token masking or POS-guided replacement\n        u = np.random.uniform()\n        if u < p_mask:\n            sentence.append(mask_token)\n        elif u < (p_mask + p_pos):\n            same_pos = pos_dict[word.pos_]\n            # Pick from list of words with same POS tag\n            sentence.append(np.random.choice(same_pos))\n        else:\n            sentence.append(word.text.lower())\n    # Apply n-gram sampling\n    if len(sentence) > 2 and np.random.uniform() < p_ng:\n        n = min(np.random.choice(range(1, 5+1)), len(sentence) - 1)\n        start = np.random.choice(len(sentence) - n)\n        for idx in range(start, start + n):\n            sentence[idx] = mask_token\n    return sentence\n\n    \ndef augmentation(sentences, pos_dict, n_iter=20, p_mask=0.1, p_pos=0.1, p_ng=0.25, max_ng=5):\n    \"\"\"\n    generates augmenetd samples for for entire training set based on three techniques described here https://blog.floydhub.com/knowledge-distillation/.\n    \"\"\"\n    augmented = []\n    for sentence in tqdm(sentences, \"Generation\"):\n        samples = [[word.text.lower() for word in sentence]]\n        for _ in range(n_iter):\n            new_sample = make_sample(sentence, pos_dict, p_mask, p_pos, p_ng, max_ng)\n            if new_sample not in samples:\n                samples.append(new_sample)\n        augmented.extend(samples)\n    return augmented\n\ndef generate_dataset_student(input_dir, output_dir, model_to_load, no_augment= False, batch_size= 16, no_cuda= False ):\n    \"\"\"\n    generates dataset for training the student (BiLSTM model). Target (class scores) for student is generated using the fine-tuned teacher (Bert)\n    \"\"\"\n    device = torch.device(\"cuda\" if not no_cuda and torch.cuda.is_available() else \"cpu\")\n    set_seed(42)\n    # Load original tsv file\n    input_tsv = load_tsv(input_dir)\n    if not no_augment:\n        sentences = [spacy_en(text) for text, _ in tqdm(input_tsv, desc=\"Loading dataset\")]\n        # build lists of words indexes by POS tab\n        pos_dict = build_pos_dict(sentences)\n        # Generate augmented samples\n        sentences = augmentation(sentences, pos_dict)\n    else:\n        sentences = [text for text, _ in input_tsv]\n\n    # Load teacher model\n    model = BertForSequenceClassification.from_pretrained(model_to_load).to(device)\n    tokenizer = BertTokenizer.from_pretrained(model_to_load, do_lower_case=True)\n\n    # Assign labels with teacher\n    teacher_field = data.Field(sequential=True, tokenize=tokenizer.tokenize, lower=True, include_lengths=True, batch_first=True)\n    fields = [(\"text\", teacher_field)]\n    if not no_augment:\n        examples = [data.Example.fromlist([\" \".join(words)], fields) for words in sentences]\n    else:\n        examples = [data.Example.fromlist([text], fields) for text in sentences]\n    augmented_dataset = data.Dataset(examples, fields)\n    teacher_field.vocab = BertVocab(tokenizer.vocab)\n    new_labels = BertTrainer(model, device, batch_size=batch_size).infer(augmented_dataset)\n\n    # Write to file\n    with open(output_dir, \"w\") as f:\n        f.write(\"sentence\\tscores\\n\")\n        for sentence, rating in zip(sentences, new_labels):\n            if not no_augment:\n                text = \" \".join(sentence)\n            else: text = sentence\n            f.write(\"%s\\t%.6f %.6f\\n\" % (text, *rating))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def save_bert(model, tokenizer, config, output_dir):\n    if not os.path.isdir(output_dir):\n        os.mkdir(output_dir)\n    config.save_pretrained(output_dir)\n    model.save_pretrained(output_dir)\n    tokenizer.save_pretrained(output_dir)\n\ndef finetune_bert_teacher(data_dir, output_dir, lr_schedule, cache_dir, epochs=1, batch_size=16, gradient_accumulation_steps=1, lr= 5e-5, warmup_steps=0, epochs_per_cycle=1, \n             do_train= False, seed=42,  no_cuda= False, checkpoint_interval=1):\n    \"\"\"\n    fine-tunes bert model (teacher) for sequence classification with original train data (class id (1/0)). This fine-tuned model will be used to generate\n    class scores for transfer set (augmented/non-augmented) which will be used to train the student model (BiLSTM in this case).\n    \"\"\"\n    if lr_schedule == \"constant\":\n        lr_schedule = None\n    device = torch.device(\"cuda\" if not no_cuda and torch.cuda.is_available() else \"cpu\")\n    set_seed(seed)\n    if not os.path.isdir(output_dir):\n        os.mkdir(output_dir)\n\n    bert_config = BertConfig.from_pretrained(\"bert-base-uncased\", cache_dir=cache_dir)\n    bert_model = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\", config=bert_config, cache_dir=cache_dir).to(device)\n    bert_tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\", do_lower_case=True, cache_dir=cache_dir)\n    #this will return data for fine-tuning the bert with original train data (class id (1/0))\n    train_dataset, valid_dataset, _ = load_data(data_dir, bert_tokenizer.tokenize,\n        vocab=BertVocab(bert_tokenizer.vocab), batch_first=True)\n    \n    #Bert is finetuned with cross-entropy loss with hard labels as target since it is a teacher model in this case.\n    #Bert outputs logits (not softmax probs)\n    trainer = BertTrainer(bert_model, device,\n        loss=\"cross_entropy\",\n        train_dataset=train_dataset,\n        val_dataset=valid_dataset, val_interval=250,\n        checkpt_callback=lambda m, step: save_bert(m, bert_tokenizer, bert_config, os.path.join(output_dir, \"checkpt_%d\" % step)), #used for saving bert model while training\n        checkpt_interval=checkpoint_interval,\n        batch_size=batch_size, gradient_accumulation_steps=gradient_accumulation_steps,\n        lr=lr)\n    if do_train:\n        trainer.train(epochs, schedule=lr_schedule,\n            warmup_steps=warmup_steps, epochs_per_cycle=epochs_per_cycle)\n\n    print(\"Evaluating bert (teacher) model:\")\n    print(trainer.evaluate())\n    #saving this model so that it can be used to generate class scores for transfer set.\n    save_bert(bert_model, bert_tokenizer, bert_config, output_dir)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class MultiChannelEmbedding(nn.Module):\n    def __init__(self, vocab_size, embed_size, filters_size=64, filters=[2, 4, 6], dropout_rate=0.0):\n        super().__init__()\n        self.vocab_size = vocab_size\n        self.embed_size = embed_size\n        self.filters_size = filters_size\n        self.filters = filters\n        self.dropout_rate = dropout_rate\n        self.embedding = nn.Embedding(self.vocab_size, self.embed_size)\n        self.conv1 = nn.ModuleList([\n            nn.Conv1d(self.embed_size, filters_size, kernel_size=f, padding=f//2)\n            for f in filters\n        ])\n        self.act = nn.Sequential(\n            nn.ReLU(inplace=True),\n            #nn.Dropout(p=dropout_rate)\n        )\n    def init_embedding(self, weight):\n        self.embedding.weight = nn.Parameter(weight.to(self.embedding.weight.device))\n    def forward(self, x):\n        x = x.transpose(0, 1)\n        x = self.embedding(x).transpose(1, 2)\n        channels = []\n        for c in self.conv1:\n            channels.append(c(x))\n        x = F.relu(torch.cat(channels, 1))\n        x = x.transpose(1, 2).transpose(0, 1)\n        return x   \n\nclass BiLSTMClassifier(nn.Module): \n    \"\"\"\n    Creates a BiLSTM classifier network for student which can be trained with augemented/non-augmented (class scores) or original train data (class id).\n    It returns Logits not the class labels/softmax probs.\n    \"\"\"\n    def __init__(self, num_classes, vocab_size, embed_size, lstm_hidden_size, classif_hidden_size,\n        lstm_layers=1, dropout_rate=0.0, use_multichannel_embedding=False):\n        super().__init__()\n        self.vocab_size = vocab_size\n        self.lstm_hidden_size = lstm_hidden_size\n        self.use_multichannel_embedding = use_multichannel_embedding\n        if self.use_multichannel_embedding:\n            self.embedding = MultiChannelEmbedding(self.vocab_size, embed_size, dropout_rate=dropout_rate)\n            self.embed_size = len(self.embedding.filters) * self.embedding.filters_size\n        else:\n            self.embedding = nn.Embedding(self.vocab_size, embed_size)\n            self.embed_size = embed_size\n        self.lstm = nn.LSTM(self.embed_size, self.lstm_hidden_size, lstm_layers, bidirectional=True, dropout=dropout_rate)\n        self.classifier = nn.Sequential(\n            nn.Linear(lstm_hidden_size*2, classif_hidden_size),\n            nn.ReLU(inplace=True),\n            nn.Dropout(p=dropout_rate),\n            nn.Linear(classif_hidden_size, num_classes)\n        )\n    def init_embedding(self, weight):\n        if self.use_multichannel_embedding:\n            self.embedding.init_embedding(weight)\n        else:\n            self.embedding.weight = nn.Parameter(weight.to(self.embedding.weight.device))\n    def forward(self, seq, length):\n        # TODO use sort_within_batch?\n        # Sort batch\n        seq_size, batch_size = seq.size(0), seq.size(1)\n        length_perm = (-length).argsort()\n        length_perm_inv = length_perm.argsort()\n        seq = torch.gather(seq, 1, length_perm[None, :].expand(seq_size, batch_size))\n        length = torch.gather(length, 0, length_perm)\n        # Pack sequence\n        seq = self.embedding(seq)\n        seq = pack_padded_sequence(seq, length)\n        # Send through LSTM\n        features, hidden_states = self.lstm(seq) #hidden_states is a tuple of (h,c) from last layer of bilstm where both the tensors (h & c ) have shape of (2, 64, 300). 2, since this is a bilstm.\n        # Unpack sequence\n        #this returns (45,64,600) for max seqlen in batch= 45, batch size=64, and lstm_hidden_size= 300 (600 because bilstm and batch_first= False)\n        features = pad_packed_sequence(features)[0]\n        # Separate last dimension into forward/backward features\n        features = features.view(seq_size, batch_size, 2, -1) #shape: (45, 64, 2, 300)\n        # Index to get forward and backward features and concatenate\n        # Gather last word for each sequence\n        #with pack_padded_sequence, we are not processing (for BiLSTM) padded tokens. That's why last hidden state for forward network will depend\n        #on length of sequence. for seq of length=30, last hidden state will be at 29th index in features Tensor.\n        #(length-1) because last index (as per length) is one less than sequence length.\n        last_indexes = (length - 1)[None, :, None, None].expand((1, batch_size, 2, features.size(-1))) #shape: (1,64,2,300)\n        forward_features = torch.gather(features, 0, last_indexes) #shape: (1,64,2,300)\n        # Squeeze seq dimension, take forward features\n        forward_features = forward_features[0, :, 0] #shape: (64,300). for each of the sequneces in batch we have 300d feature from last valid (non-padded) timestamp.\n        # Take first word, backward features\n        #last hidden state for backward lstm is always found at 0th index of the sequence irrespective of sequence length.\n        backward_features = features[0, :, 1]  #shape: (64,300). for each of the sequneces in batch we have 300d feature from first timestamp for backward network.\n        features = torch.cat((forward_features, backward_features), -1) #shape: (64,600). concatenated both forward and bakcward features.\n        # Send through classifier\n        logits = self.classifier(features) #shape: (64,2). for each seq in batch we have 2d logits score since we are dealing with 2 classes only.\n        #Batch was sorted by length of each sequence. Before outputting it has to be brought to original order.\n        # Invert batch permutation\n        logits = torch.gather(logits, 0, length_perm_inv[:, None].expand((batch_size, logits.size(-1)))) #shape: (64,2) but sorted as per original order\n        return logits, hidden_states\n\ndef save_bilstm(model, output_dir):\n    if not os.path.isdir(output_dir):\n        os.mkdir(output_dir)\n    torch.save(model.state_dict(), os.path.join(output_dir, \"weights.pth\"))\n\n\ndef train_bilstm_student(data_dir, output_dir, augmented= False, use_teacher=False, epochs=1, batch_size=64, gradient_accumulation_steps=1,\n                        lr= 5e-5, lr_schedule= \"constant\", warmup_steps=0, epochs_per_cycle_bilstm_bilstm=1,  do_train= False, seed=42, checkpoint_interval= -1, no_cuda= False):\n\n    if not os.path.isdir(output_dir):\n        os.mkdir(output_dir)\n    device = torch.device(\"cuda\" if not no_cuda and torch.cuda.is_available() else \"cpu\")\n    set_seed(seed)\n    #loading data for training the BiLSTM classifier based on provided input arguments\n    train_dataset, valid_dataset, text_field = load_data(data_dir, spacy_tokenizer, augmented=augmented, use_teacher=use_teacher)\n    vocab = text_field.vocab\n    \n    #creating an instance of a BiLSTMClassifier network\n    model = BiLSTMClassifier(2, len(vocab.itos), vocab.vectors.shape[-1],\n        lstm_hidden_size=300, classif_hidden_size=400, dropout_rate=0.15).to(device)\n    # Initialize word embeddings to fasttext\n    model.init_embedding(vocab.vectors.to(device)) #vocab.vectors is a 2D tensor of shape (vocab_size, embed_size). it initilaizes weight of Embedding layer.\n    \n    #when either augmented or use_teacher is TRUE:\n        #then BiLSTM (student) is being trained with mse loss (not KlDiv loss) with target as class scores from Bert (teacher) since as T becomes larger \n        #the Kullback–Leibler divergence becomes more and more similar to applying MSE Loss to the raw scores. MSE Loss tends to be more common for \n        #training small networks since, among a variety of reasons, it doesn’t have hyper-parameters. That is, we don’t need to pick a value for T. We\n        #can also use klDiv loss in this case for comparison but T has to be fine-tuned as an hyper-param.\n    #when both augmented and use_teacher are False:\n        #then it indicates that BiLSTM has to be trained in isolation (not with the teacher) using hard labels\n        #as targets. That's why it has to be trained with cross-entropy loss in this case.\n    trainer = LSTMTrainer(model, device,\n        loss=\"mse\" if augmented or use_teacher else \"cross_entropy\",\n        train_dataset=train_dataset, val_dataset=valid_dataset, val_interval=250,\n        checkpt_interval=checkpoint_interval,\n        checkpt_callback=lambda m, step: save_bilstm(m, os.path.join(output_dir, \"checkpt_%d\" % step)), #used for saving bilstm model while training\n        batch_size=batch_size, gradient_accumulation_steps=gradient_accumulation_steps,\n        lr=lr)\n    \n    if do_train:\n        trainer.train(epochs, schedule=lr_schedule,\n            warmup_steps=warmup_steps, epochs_per_cycle_bilstm=epochs_per_cycle_bilstm)\n    \n    print(\"Evaluating model:\")\n    print(trainer.evaluate())\n    #save trained bilstm model which can be used for generating the class labels for test data\n    save_bilstm(model, output_dir)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class Trainer():\n    def __init__(self, model, device,\n        loss=\"cross_entropy\",\n        train_dataset=None,\n        temperature=1.0,\n        val_dataset=None, val_interval=1,\n        checkpt_callback=None, checkpt_interval=1,\n        max_grad_norm=1.0, batch_size=64, gradient_accumulation_steps=1,\n        lr=5e-5, weight_decay=0.0):\n        # Storing\n        self.model = model\n        self.device = device\n        self.loss_option = loss\n        self.train_dataset = train_dataset\n        self.temperature = temperature\n        self.val_dataset = val_dataset\n        self.val_interval = val_interval\n        self.checkpt_callback = checkpt_callback\n        self.checkpt_interval = checkpt_interval\n        self.max_grad_norm = max_grad_norm\n        self.batch_size = batch_size\n        self.gradient_accumulation_steps = gradient_accumulation_steps\n        self.lr = lr\n        self.weight_decay = weight_decay\n        # Initialization\n        assert self.loss_option in [\"cross_entropy\", \"mse\", \"kl_div\"]\n        if self.loss_option == \"cross_entropy\":\n            self.loss_function = nn.CrossEntropyLoss(reduction=\"sum\")\n        elif self.loss_option == \"mse\":\n            self.loss_function = nn.MSELoss(reduction=\"sum\")\n        elif self.loss_option == \"kl_div\":\n            self.loss_function = nn.KLDivLoss(reduction=\"sum\")\n        self.optimizer = torch.optim.Adam(self.model.parameters(), lr=self.lr, weight_decay=self.weight_decay)\n        if self.train_dataset is not None:\n            self.train_it = data.BucketIterator(self.train_dataset, self.batch_size, train=True, sort_key=lambda x: len(x.text), device=self.device)\n        else:\n            self.train_it = None\n        if self.val_dataset is not None:\n            self.val_it = data.BucketIterator(self.val_dataset, self.batch_size, train=False, sort_key=lambda x: len(x.text), device=self.device)\n        else:\n            self.val_it = None\n    def get_loss(self, model_output, label, curr_batch_size):\n        #cross-entropy loss is used with the hard labels (class ids) as target but mse and Kullback–Leibler divergence loss are \n        #used with the teacher labels (class scores) as target.\n        #CrossEntropyLoss and MSELoss takes logits not the softmax output (softmax probs).\n        #You should pass raw logits to nn.CrossEntropyLoss, since the function itself applies F.log_softmax and nn.NLLLoss() on the input.\n        #If you pass log probabilities (from nn.LogSoftmax) or probabilities (from nn.Softmax()) your loss function won’t work as intended.\n        if self.loss_option in [\"cross_entropy\", \"mse\"]:\n            loss = self.loss_function(\n                model_output, #this is a logit output from bert or bilstm model\n                label #hard label for cross-entropy and class scores from Bert (teacher) for mse\n            ) / curr_batch_size # Mean over batch\n        elif self.loss_option == \"kl_div\":\n            # KL Divergence loss needs special care\n            # It expects log probabilities for the model's output, and probabilities for the label\n            loss = self.loss_function(\n                F.log_softmax(model_output / self.temperature, dim=-1),\n                F.softmax(label / self.temperature, dim=-1)\n            ) / (self.temperature * self.temperature) / curr_batch_size\n        return loss\n    def train_step(self, batch):\n        self.model.train() #put the model in train mode\n        batch, label, curr_batch_size = self.process_batch(batch)\n        #model output is a tuple, that's why indexing for 0 to fetch the logit outputs.\n        #since batch is a keyworded argument (return from process_batch function), that's why ** for unrolling.\n        s_logits = self.model(**batch)[0] #both Bert and BiLSTM model here outputs logits (not softmax probs)\n        loss = self.get_loss(s_logits, label, curr_batch_size)\n        loss.backward()\n        self.training_step += 1\n        #keep accumulating the gradients for gradient_accumulation_steps steps then update the weights and lr (if needed) and set the gradient to zero.\n        if self.training_step % self.gradient_accumulation_steps == 0:\n            # Apply gradient clipping\n            nn.utils.clip_grad_norm_(self.model.parameters(), self.max_grad_norm)\n            self.optimizer.step()\n            if self.scheduler is not None:\n                # Advance learning rate schedule\n                #scheduler.step() updates learning rate as per the scheduler, while optimizer.step() performs on batch-level which updates parameters.\n                self.scheduler.step()\n            self.model.zero_grad()\n            # Save stats to tensorboard\n            self.tb_writer.add_scalar(\"lr\",\n                self.scheduler.get_lr()[0] if self.scheduler is not None else self.lr,\n                self.global_step)\n            self.tb_writer.add_scalar(\"loss\", loss, self.global_step)\n            self.global_step += 1 #have doubts here regarding global_step being incremented here.\n            # Every val_interval steps, evaluate and log stats to tensorboard\n            if self.val_interval >= 0 and (self.global_step + 1) % self.val_interval == 0: # doubt: if global_step already incremented then why we are using global_step+1.\n                results = self.evaluate()\n                print(results)\n                for k, v in results.items():\n                    self.tb_writer.add_scalar(\"val_\" + k, v, self.global_step)\n            # Every checkpt_interval steps, call checkpt_callback to save a checkpoint\n            if self.checkpt_interval >= 0 and (self.global_step + 1) % self.checkpt_interval == 0:\n                self.checkpt_callback(self.model, self.global_step)\n    def train(self, epochs=1, schedule=None, **kwargs):\n        # Initialization\n        self.global_step = 0 #tracks only the parameter updation steps\n        self.training_step = 0 #tracks all the gradient calculation steps\n        self.tb_writer = SummaryWriter()\n        steps_per_epoch = len(self.train_dataset) // self.batch_size // self.gradient_accumulation_steps\n        total_steps = epochs * steps_per_epoch\n        # Initialize the learning rate scheduler if one has been chosen\n        assert schedule is None or schedule in [\"warmup\", \"cyclic\"]\n        if schedule is None:\n            self.scheduler = None\n            for grp in self.optimizer.param_groups: grp['lr'] = self.lr\n        elif schedule == \"warmup\":\n            warmup_steps = kwargs[\"warmup_steps\"]\n            self.scheduler = torch.optim.lr_scheduler.CyclicLR(self.optimizer, base_lr=self.lr/100, max_lr=self.lr,\n                step_size_up=max(1, warmup_steps), step_size_down=(total_steps - warmup_steps),cycle_momentum=False)\n        elif schedule == \"cyclic\":\n            epochs_per_cycle = kwargs[\"epochs_per_cycle\"]\n            self.scheduler = torch.optim.lr_scheduler.CyclicLR(self.optimizer, base_lr=self.lr/25, max_lr=self.lr,\n            step_size_up=steps_per_epoch // 2, cycle_momentum=False)\n        # The loop over epochs, batches\n        for epoch in trange(epochs, desc=\"Training\"):\n            for batch in tqdm(self.train_it, desc=\"Epoch %d\" % epoch):\n                self.train_step(batch)\n        self.tb_writer.close()\n        del self.tb_writer\n    \n    def evaluate(self):\n        '''\n        Evaluate the model on dev/valid data.\n        '''\n        self.model.eval() #put the model in eval mode\n        val_loss = val_accuracy = 0.0\n        loss_func = nn.CrossEntropyLoss(reduction=\"sum\")\n        for batch in tqdm(self.val_it, desc=\"Evaluation\"):\n            with torch.no_grad():\n                batch, label, _ = self.process_batch(batch)\n                output = self.model(**batch)[0] #both Bert and BiLSTM model here outputs logits (not softmax probs)\n                loss = loss_func(output, label)\n                val_loss += loss.item()\n                val_accuracy += (output.argmax(dim=-1) == label).sum().item()\n        val_loss /= len(self.val_dataset)\n        val_accuracy /= len(self.val_dataset)\n        return {\n            \"loss\": val_loss,\n            \"perplexity\": np.exp(val_loss),\n            \"accuracy\": val_accuracy\n        }\n    # get the inference on any \n    def infer(self, dataset, softmax=False):\n        '''\n        Get the inference for any new data.\n        '''\n        self.model.eval() #put the model in eval mode\n        outputs_idx = 0\n        outputs = np.empty(shape=(len(dataset), 2))\n        infer_it = data.Iterator(dataset, self.batch_size, train=False, sort=False, device=self.device)\n        for batch in tqdm(infer_it, desc=\"Inference\"):\n            with torch.no_grad():\n                batch, _, batch_size = self.process_batch(batch)\n                output = self.model(**batch)[0] #both Bert and BiLSTM model here outputs logits (not softmax probs)\n                #if softmax is True then we return softmax probs instead of logit outputs directly from Bert or BilSTM.\n                if softmax:\n                    output = F.softmax(output, dim=-1)\n                outputs[outputs_idx:outputs_idx + batch_size] = output.detach().cpu().numpy()\n                outputs_idx += batch_size\n                del output\n        return outputs\n    def infer_one(self, example, text_field=None, softmax=False):\n        self.model.eval()\n        if text_field is None:\n            text_field = self.train_dataset.fields[\"text\"]\n        example = text_field.preprocess(example)\n        tokens, length  = text_field.process([example])\n        with torch.no_grad():\n            batch = self.process_one(tokens, length)\n            output = self.model(**batch)[0]\n            if softmax:\n                output = F.softmax(output, dim=-1)\n            output = output.detach().cpu().numpy()\n        return output[0]\n    def process_batch(self, *args):\n        # Implemented by subclasses\n        raise NotImplementedError()\n    def process_one(self, *args):\n        # Implemented by subclasses\n        raise NotImplementedError()\n\nclass BertTrainer(Trainer):\n    def process_batch(self, batch):\n        #for both bert and lstm the dataset (batch) have two keys as dict Field: text and label. \n        #Here text Field is a tuple of tokens (seq of vocab indices for each sentence) and corresponding length of these sequences.\n        #for a batch size of 64 and max sequence length in that batch=54; tokens.shape: (64,54), length.shape= (64,), label.shape= (64,), attention_mask.shape= (64,54)\n        tokens, length = batch.text\n        label = batch.label if \"label\" in batch.__dict__ else None\n        length = length.unsqueeze_(1).expand(tokens.size())\n        rg = torch.arange(tokens.size(1), device=self.device).unsqueeze_(0).expand(tokens.size())\n        #attention_mask in bert is used to avoid performing attention on padding token indices. \n        #Mask values selected in [0, 1]: 1 for tokens that are NOT MASKED, 0 for MASKED tokens.\n        #attention_mask only consists of 0 and 1. Wherever token is there then attention_mask=1 and wherever token=0 (padded) then attention_mask=0\n        #shape of both tokens and attention_mask must be equal which is (batch_size X max(seq length in that batch))\n        attention_mask = (rg < length).type(torch.float32)\n        batch = {\n            \"input_ids\": tokens,\n            \"attention_mask\": attention_mask\n        }\n        return batch, label, tokens.size(0)  #tokens.size(0) because for bilstm batch_first= True\n    def process_one(self, tokens, length):\n        return {\n            \"input_ids\": tokens.to(self.device),\n            \"attention_mask\": torch.ones(tokens.size(), dtype=torch.float32, device=self.device)\n        }\n\nclass LSTMTrainer(Trainer):\n    def process_batch(self, batch):\n        tokens, length = batch.text\n        label = batch.label if \"label\" in batch.__dict__ else None\n        batch = {\n            \"seq\": tokens,\n            \"length\": length,\n        }\n        return batch, label, tokens.size(1) #tokens.size(1) because for bilstm batch_first= False\n    def process_one(self, tokens, length):\n        return {\n            \"seq\": tokens.to(self.device),\n            \"length\": length.to(self.device)\n        }\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# if __name__ == \"__main__\":\n\ndata_dir= \"../input/the-stanford-sentiment-treebank-v2-sst2/SST-2\" #Directory containing the dataset (tsv files).\noutput_dir= \"../output/\" #Directory where to save the model and other outputs.\ninput_dir= \"../input/the-stanford-sentiment-treebank-v2-sst2/SST-2/train.tsv\" #location for Input dataset.\noutput_dir_student= \"../output/augmented.tsv\" #location to save the dataset for student model. if augmented= True then augmented.tsv else noaugmented.tsv\nlr_schedule_bert= \"warmup\" # Schedule to use for the learning rate. Choices are: constant, linear warmup & decay, cyclic. must be one of these {constant,warmup,cyclic}\nlr_schedule_bilstm= \"warmup\"\ncache_dir= \"/kaggle/working/\" #Custom cache for transformer models\nepochs_bert= 1 #no of epochs to finetune/train bert for\nepochs_bilstm= 1 #no of epochs to train bilstm for\nbatch_bert= 16 #batch size for bert\nbatch_bilstm= 50 #batch size for bilstm\ngradient_accumulation_steps_bert= 1 #no of steps to accumulate gradient for before parameter updation for bert\ngradient_accumulation_steps_bilstm= 1 #no of steps to accumulate gradient for before parameter updation for bilstm\nlr_bert= 1e-5\nlr_bilstm= 1e-3\nwarmup_steps_bert= 100 # Warmup steps for the 'warmup' learning rate schedule, Ignored otherwise. no of steps to increase lr for before starting to decrease it for bert\nwarmup_steps_bilstm= 100 #no of steps to increase lr for before starting to decrease it for bilstm\ndo_train_bert= True #do you want to train/finetune bert (teacher) with hard labels\ndo_train_bilstm= True #do you want to train bilstm (student) with soft labels (class scores from bert)\nepochs_per_cycle_bert= 1 #Epochs per cycle for the 'cyclic' learning rate schedule. Ignored otherwise\nepochs_per_cycle_bilstm= 1 #Epochs per cycle for the 'cyclic' learning rate schedule. Ignored otherwise\nseed= 42 #seed for reproducing same output\nno_cuda= False #if running on GPU then False else True.\ncheckpoint_interval= 1 #Interval for keep saving the models\naugmented= False #whether to use augmented or non-augmented data for knowledge distillation. True for augmented else False.\nuse_teacher= False #whether to use teacher model or train in isolation. True for teacher and False for isolation\nmodel= \"../output/\" #Model to use to generate the labels for the augmented dataset. Directory which contains the finetuned weights from bert.\nno_augment= False #whether to generate augmented dataset for student or not. if False then generate augmented data else don't generate.\n\n\nfinetune_bert_teacher(data_dir, output_dir, lr_schedule_bert, cache_dir, epochs_bert, batch_bert, gradient_accumulation_steps_bert, lr_bert, warmup_steps_bert, \n                      epochs_per_cycle_bert, do_train_bert, seed,  no_cuda, checkpoint_interval)\n\n# If dataset that would be used by student (augmented.tsv or noaugmented.tsv) is already generated or stored, then we need not run following function\ngenerate_dataset_student(input_dir, output_dir_student, model, no_augment, batch_bert, no_cuda)\n\ntrain_bilstm_student(data_dir, output_dir, augmented, use_teacher, epochs_bilstm, batch_bilstm, gradient_accumulation_steps_bilstm,\n                        lr_bilstm, lr_schedule_bilstm, warmup_steps_bilstm, epochs_per_cycle_bilstm, do_train_bilstm, seed, checkpoint_interval, no_cuda)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# import spacy\n\n# nlp = spacy.load(\"en_core_web_sm\")\n# doc = nlp(\"Apple is looking at buying U.K. startup for $1 billion\")\n# print(doc)\n# for token in doc:\n#     print(token.text, token.lemma_, token.pos_, token.tag_, token.dep_,\n#             token.shape_, token.is_alpha, token.is_stop)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# if True:\n#     sentences = [spacy_en(text) for text, _ in tqdm(input_tsv, desc=\"Loading dataset\")]\n#     # build lists of words indexes by POS tab\n#     pos_dict = build_pos_dict(sentences)\n#     # Generate augmented samples\n#     sentences = augmentation(sentences, pos_dict)\n# else:\n#     sentences = [text for text, _ in input_tsv]\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# device= \"cpu\"\n# bert_config = BertConfig.from_pretrained(\"bert-large-uncased\")\n# bert_model = BertForSequenceClassification.from_pretrained(\"bert-large-uncased\", config=bert_config).to(device)\n# bert_tokenizer = BertTokenizer.from_pretrained(\"bert-large-uncased\", do_lower_case=True)\n# train_dataset, valid_dataset, _ = load_data(\"../input/the-stanford-sentiment-treebank-v2-sst2/SST-2\", bert_tokenizer.tokenize,\n#     vocab=BertVocab(bert_tokenizer.vocab), batch_first=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# train_dataset, valid_dataset, text_field = load_data(\"../input/the-stanford-sentiment-treebank-v2-sst2/SST-2\", spacy_tokenizer)\n# vocab = text_field.vocab","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# train_it = data.BucketIterator(train_dataset,64, train=True, sort_key=lambda x: len(x.text), device=device)\n# i=1\n# for epoch in trange(1, desc=\"Training\"):\n#     for batch in tqdm(train_it, desc=\"Epoch %d\" % epoch):\n#         batch1= batch\n#         if(i==1): break\n            ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# tokens, length = batch1.text\n# label = batch.label if \"label\" in batch.__dict__ else None\n# print(length)\n# # length = length.unsqueeze(1).expand(tokens.size())\n# # length, length.size()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# seq= tokens\n# seq_size, batch_size = seq.size(0), seq.size(1)\n# length_perm = (-length).argsort()\n# length_perm_inv = length_perm.argsort()\n# seq = torch.gather(seq, 1, length_perm[None, :].expand(seq_size, batch_size))\n# length = torch.gather(length, 0, length_perm)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# embedding = nn.Embedding(vocab.vectors.shape[0], vocab.vectors.shape[1])\n# embedding.weight = nn.Parameter(vocab.vectors.to(device))\n# seq = embedding(seq)\n# seq = pack_padded_sequence(seq, length)\n# seq","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# lstm = nn.LSTM(300, 300, 1, bidirectional=True, dropout=0.0)\n# features, hidden_states = lstm(seq)\n# features = pad_packed_sequence(features)[0]\n# print(features.shape)\n# features = features.view(seq_size, batch_size, 2, -1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# last_indexes = (length - 1)[None, :, None, None].expand((1, batch_size, 2, features.size(-1)))\n# forward_features = torch.gather(features, 0, last_indexes)\n# forward_features = forward_features[0, :, 0]\n# # Take first word, backward features\n# backward_features = features[0, :, 1]\n# features = torch.cat((forward_features, backward_features), -1)\n# forward_features.shape, backward_features.shape\n# features.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# classifier = nn.Sequential(\n#             nn.Linear(600, 300),\n#             nn.ReLU(inplace=True),\n#             nn.Dropout(p=0.0),\n#             nn.Linear(300, 2)\n#         )\n# logits = classifier(features)\n# logits = torch.gather(logits, 0, length_perm_inv[:, None].expand((64, logits.size(-1))))\n# logits.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# (length-1)[None, :, None, None].expand((1, batch_size, 2, features.size(-1))).shape, forward_features.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# seq_size, batch_size, length_perm[None, :].expand(seq_size, batch_size)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# rg = torch.arange(tokens.size(1), device=device).unsqueeze_(0).expand(tokens.size())\n# # print(rg, rg.shape)\n# attention_mask = (rg < length).type(torch.float32)\n# batch = {\n#             \"input_ids\": tokens,\n#             \"attention_mask\": attention_mask\n#         }\n# # attention_mask, attention_mask.shape\n# batch","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# # device= \"cuda\"\n# # model = BertForSequenceClassification.from_pretrained(\"bert-large-uncased\").to(device)\n# # tokenizer = BertTokenizer.from_pretrained(\"bert-large-uncased\", do_lower_case=True)\n\n# # # Assign labels with teacher\n# # teacher_field = data.Field(sequential=True, tokenize=tokenizer.tokenize, lower=True, include_lengths=True, batch_first=True)\n# # fields = [(\"text\", teacher_field)]\n# # if True:\n# #     examples = [data.Example.fromlist([\" \".join(words)], fields) for words in sentences]\n# # else:\n# #     examples = [data.Example.fromlist([text], fields) for text in sentences]\n# # augmented_dataset = data.Dataset(examples, fields)\n# # teacher_field.vocab = BertVocab(tokenizer.vocab)\n# new_labels = BertTrainer(model, device, batch_size=args.batch_size).infer(augmented_dataset)\n# # Write to file\n# with open(args.output, \"w\") as f:\n#     f.write(\"sentence\\tscores\\n\")\n#     for sentence, rating in zip(sentences, new_labels):\n#         if not args.no_augment:\n#             text = \" \".join(sentence)\n#         else: text = sentence\n#         f.write(\"%s\\t%.6f %.6f\\n\" % (text, *rating))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# augmented_dataset.examples[1].__dict__.keys()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# bert_tokenizer.vocab[\"apple\"]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# vocab=BertVocab(bert_tokenizer.vocab)\n# vocab[\"naquib\"]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# for name, layer in model.named_parameters():\n#     print(name, layer.shape, layer)\n# [print(\"nnnn\", layer) for idx, layer in enumerate(model.children())]\n# for name, module in model.named_modules():\n#     print(name)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# length= torch.LongTensor([5,3,8,4,7,12,43,21,1])\n# length_perm = (-length).argsort() \n# length_perm","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# length_perm[None, :].expand(9, 9)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.7.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":4}